{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "from agents.base_agent import Agent\n",
    "from agents.random_agent import RandomAgent\n",
    "from agents.sac import SACAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Christian\\uni\\RLP\\city_learn_2023\\.venv\\lib\\site-packages\\citylearn\\dynamics.py:102: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(self.filepath)['model_state_dict'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "-44620.4802984519\n",
      "Episode 1/3, Total Reward: -44620.4802984519\n",
      "29\n",
      "-86011.0707178354\n",
      "Episode 2/3, Total Reward: -41390.59041938351\n",
      "29\n",
      "-129099.5891906464\n",
      "Episode 3/3, Total Reward: -43088.51847281098\n"
     ]
    }
   ],
   "source": [
    "decentralized_observation_space_dim = 29\n",
    "decentralized_action_space_dim = 6 \n",
    "building_number = 3\n",
    "    \n",
    "    # Initialize SAC Agent\n",
    "\n",
    "agents = []\n",
    "for i in range(building_number):\n",
    "    agents.append(\n",
    "        SACAgent(\n",
    "            observation_space_dim=decentralized_observation_space_dim, \n",
    "            action_space_dim=decentralized_action_space_dim,\n",
    "            hidden_dim=256,\n",
    "            buffer_size=100000,\n",
    "            batch_size=256,\n",
    "            learning_rate=3e-4,\n",
    "            gamma=0.99,\n",
    "            tau=0.01,\n",
    "            alpha=0.05,\n",
    "            action_space=decentralized_env.action_space,\n",
    "            exploration_timesteps = 0\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "    # Train the agent\n",
    "rewards = train_sac_agent(decentralized_env, agents, episodes=3, decentralized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sac_agent(\n",
    "    env: CityLearnEnv, \n",
    "    agents: list[SACAgent], \n",
    "    episodes: int = 100, \n",
    "    decentralized = False\n",
    ") -> None:\n",
    "    \"\"\"Train SAC agent in the environment\"\"\"\n",
    "    total_reward = 0\n",
    "    \n",
    "    reward_list = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        # Reset environment and get initial observation\n",
    "        observation = env.reset()\n",
    "        print(len(observation[0]))\n",
    "\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not env.done:\n",
    "            if not decentralized:\n",
    "                flat_observation = np.concatenate(observation) if isinstance(observation, list) else observation\n",
    "\n",
    "            \n",
    "            # select actions based on different paradigms\n",
    "            if decentralized:\n",
    "                actions = [0 for _ in range(len(agents))]\n",
    "                \n",
    "                for i in range(len(agents)):\n",
    "                    # agent_actions is used for the replay buffer\n",
    "                    actions[i] = agents[i].select_action(observation[i]).tolist()\n",
    "                    # ADD THIS TO REPLAY BUFFER SAMPLING ASW\n",
    "            else:\n",
    "                actions = [agents[0].select_action(flat_observation).tolist()]\n",
    "            \n",
    "            #print(f\"actions: {actions}\") # action is a list of lists (one for each agent) of actions)\n",
    "            for agent in agents:\n",
    "                agent.total_steps += 1\n",
    "                    \n",
    "            next_observation, reward, info, done = env.step(actions)\n",
    "            \n",
    "            reward_list.append(reward)\n",
    "\n",
    "            if not decentralized:       \n",
    "                flat_next_observation = np.concatenate(next_observation) if isinstance(next_observation, list) else next_observation\n",
    "            \n",
    "            episode_reward += np.sum(reward)\n",
    "\n",
    "            if decentralized:\n",
    "                for i in range(len(agents)):\n",
    "                    agents[i].replay_buffer.push(\n",
    "                        observation[i], \n",
    "                        actions[i], \n",
    "                        np.sum(reward),\n",
    "                        next_observation[i], \n",
    "                        len(done)\n",
    "                    )\n",
    "            else:               \n",
    "                agents[0].replay_buffer.push(\n",
    "                    flat_observation, \n",
    "                    actions, \n",
    "                    np.sum(reward),\n",
    "                    flat_next_observation, \n",
    "                    len(done)\n",
    "                )\n",
    "            \n",
    "            if agent.total_steps >= agent.exploration_timesteps:\n",
    "                agent.train()\n",
    "            \n",
    "            \n",
    "            observation = next_observation\n",
    "        \n",
    "        total_reward += episode_reward\n",
    "        print(total_reward)\n",
    "        \n",
    "        print(f\"Episode {episode+1}/{episodes}, Total Reward: {episode_reward}\")\n",
    "    \n",
    "    return reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Christian\\uni\\RLP\\city_learn_2023\\.venv\\lib\\site-packages\\citylearn\\dynamics.py:102: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(self.filepath)['model_state_dict'])\n"
     ]
    }
   ],
   "source": [
    "SEED = 0\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "root_directory = Path(\"../data/citylearn_challenge_2023_phase_1\")\n",
    "schema_path = root_directory / \"schema.json\"\n",
    "\n",
    "centralized_env = CityLearnEnv(\n",
    "    schema=schema_path,\n",
    "    root_directory=root_directory,\n",
    "    random_seed=SEED,\n",
    "    central_agent=True,\n",
    ")\n",
    "\n",
    "decentralized_env = CityLearnEnv(\n",
    "    schema=schema_path,\n",
    "    root_directory=root_directory,\n",
    "    random_seed=SEED,\n",
    "    central_agent=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centralized_centralized_observation_space_dim = 49 \n",
    "centralized_action_space_dim = 18 # set to 18 and turn on other actions\n",
    "    \n",
    "    \n",
    "    # Initialize SAC Agent\n",
    "sac_agent = SACAgent(\n",
    "    observation_space_dim=centralized_centralized_observation_space_dim, \n",
    "    action_space_dim=centralized_action_space_dim,\n",
    "    hidden_dim=256,\n",
    "    buffer_size=100000,\n",
    "    batch_size=256,\n",
    "    learning_rate=3e-4,\n",
    "    gamma=0.99,\n",
    "    tau=0.01,\n",
    "    alpha=0.05,\n",
    "    action_space=env.action_space,\n",
    "    exploration_timesteps = 0\n",
    ")\n",
    "\n",
    "\n",
    "    \n",
    "    # Train the agent\n",
    "rewards = train_sac_agent(centralized_env, sac_agent, episodes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centralized_interact_with_env(\n",
    "    env: CityLearnEnv, agent: Agent = RandomAgent, episodes: int = 100\n",
    ") -> None:\n",
    "    \"\"\"Interact with environment using agent\"\"\"\n",
    "    reward_list = []\n",
    "    for episode in range(episodes):\n",
    "        observation = env.reset()\n",
    "        while not env.done:\n",
    "            action = agent.select_action(observation)\n",
    "            observation, reward, info, done = env.step(action)\n",
    "            reward_list.append(reward)\n",
    "    \n",
    "    return reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decentralized_interact_with_env(\n",
    "    env: CityLearnEnv, agent: Agent = RandomAgent, episodes: int = 100\n",
    ") -> None:\n",
    "    \"\"\"Interact with environment using agent\"\"\"\n",
    "    reward_list = []\n",
    "    for episode in range(episodes):\n",
    "        observation = env.reset()\n",
    "        while not env.done:\n",
    "            action = agent.select_action(observation)\n",
    "            observation, reward, info, done = env.step(action)\n",
    "            reward_list.append(reward)\n",
    "    \n",
    "    return reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_agent = RandomAgent(env.observation_space, env.action_space)\n",
    "# random_rewards = centralized_interact_with_env(env, random_agent, episodes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat = np.concatenate(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('inline')\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(flat, 'k')\n",
    "plt.savefig(\"test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def format_evaluation(evaluation_data: dict) -> pd.DataFrame:\n",
    "    kpis = pd.DataFrame.from_dict(\n",
    "        evaluation_data, orient=\"index\", columns=[\"value\", \"display_name\", \"weight\"]\n",
    "    )\n",
    "    kpis_reset = kpis.reset_index().rename(columns={\"index\": \"metric\"})\n",
    "    return kpis_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "      <th>display_name</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>carbon_emissions_total</td>\n",
       "      <td>0.885068</td>\n",
       "      <td>Carbon emissions</td>\n",
       "      <td>0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>discomfort_proportion</td>\n",
       "      <td>0.886691</td>\n",
       "      <td>Unmet hours</td>\n",
       "      <td>0.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ramping_average</td>\n",
       "      <td>1.438163</td>\n",
       "      <td>Ramping</td>\n",
       "      <td>0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>daily_one_minus_load_factor_average</td>\n",
       "      <td>1.112994</td>\n",
       "      <td>Load factor</td>\n",
       "      <td>0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>daily_peak_average</td>\n",
       "      <td>0.930902</td>\n",
       "      <td>Daily peak</td>\n",
       "      <td>0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>annual_peak_average</td>\n",
       "      <td>0.867638</td>\n",
       "      <td>All-time peak</td>\n",
       "      <td>0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>one_minus_thermal_resilience_proportion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thermal resilience</td>\n",
       "      <td>0.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>power_outage_normalized_unserved_energy_total</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unserved energy</td>\n",
       "      <td>0.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>average_score</td>\n",
       "      <td>0.680741</td>\n",
       "      <td>Score</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          metric     value  \\\n",
       "0                         carbon_emissions_total  0.885068   \n",
       "1                          discomfort_proportion  0.886691   \n",
       "2                                ramping_average  1.438163   \n",
       "3            daily_one_minus_load_factor_average  1.112994   \n",
       "4                             daily_peak_average  0.930902   \n",
       "5                            annual_peak_average  0.867638   \n",
       "6        one_minus_thermal_resilience_proportion       NaN   \n",
       "7  power_outage_normalized_unserved_energy_total       NaN   \n",
       "8                                  average_score  0.680741   \n",
       "\n",
       "         display_name  weight  \n",
       "0    Carbon emissions   0.100  \n",
       "1         Unmet hours   0.300  \n",
       "2             Ramping   0.075  \n",
       "3         Load factor   0.075  \n",
       "4          Daily peak   0.075  \n",
       "5       All-time peak   0.075  \n",
       "6  Thermal resilience   0.150  \n",
       "7     Unserved energy   0.150  \n",
       "8               Score     NaN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_evaluation(env.evaluate_citylearn_challenge())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
