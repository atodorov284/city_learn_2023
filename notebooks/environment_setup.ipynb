{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from citylearn.agents.base import Agent as RandomAgent\n",
    "from citylearn.agents.rbc import HourRBC\n",
    "from citylearn.agents.sac import SAC\n",
    "from citylearn.agents.q_learning import TabularQLearning\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'reward_function'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m root_directory \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/citylearn_challenge_2023_phase_1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m schema_path \u001b[38;5;241m=\u001b[39m root_directory \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mschema.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mCityLearnEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot_directory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Uni Stuff\\Bachelor\\RL\\Challenges\\city_learn_2023\\.venv\\lib\\site-packages\\citylearn\\citylearn.py:104\u001b[0m, in \u001b[0;36mCityLearnEnv.__init__\u001b[1;34m(self, schema, root_directory, buildings, simulation_start_time_step, simulation_end_time_step, episode_time_steps, rolling_episode_split, random_episode_split, seconds_per_time_step, reward_function, central_agent, shared_observations, random_seed, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_seed \u001b[38;5;241m=\u001b[39m random_seed\n\u001b[0;32m    103\u001b[0m root_directory, buildings, episode_time_steps, rolling_episode_split, random_episode_split, \\\n\u001b[1;32m--> 104\u001b[0m     seconds_per_time_step, reward_function, central_agent, shared_observations, episode_tracker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuildings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuildings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43msimulation_start_time_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msimulation_start_time_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43msimulation_end_time_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msimulation_end_time_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepisode_time_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisode_time_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrolling_episode_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrolling_episode_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_episode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_episode_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseconds_per_time_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseconds_per_time_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreward_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreward_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcentral_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcentral_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshared_observations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshared_observations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_directory \u001b[38;5;241m=\u001b[39m root_directory\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuildings \u001b[38;5;241m=\u001b[39m buildings\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Uni Stuff\\Bachelor\\RL\\Challenges\\city_learn_2023\\.venv\\lib\\site-packages\\citylearn\\citylearn.py:1377\u001b[0m, in \u001b[0;36mCityLearnEnv._load\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   1375\u001b[0m     reward_function_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(reward_function_type\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m   1376\u001b[0m     reward_function_name \u001b[38;5;241m=\u001b[39m reward_function_type\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m-> 1377\u001b[0m     reward_function_constructor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreward_function_module\u001b[49m\u001b[43m)\u001b[49m, reward_function_name)\n\u001b[0;32m   1378\u001b[0m     reward_function \u001b[38;5;241m=\u001b[39m reward_function_constructor(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mreward_function_attributes)\n\u001b[0;32m   1380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m   1381\u001b[0m     root_directory, buildings, episode_time_steps, rolling_episode_split, random_episode_split, \n\u001b[0;32m   1382\u001b[0m     seconds_per_time_step, reward_function, central_agent, shared_observations, episode_tracker\n\u001b[0;32m   1383\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\uv\\python\\cpython-3.10.15-windows-x86_64-none\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1004\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'reward_function'"
     ]
    }
   ],
   "source": [
    "root_directory = Path(\"../data/citylearn_challenge_2023_phase_1\")\n",
    "schema_path = root_directory / \"schema.json\"\n",
    "env = CityLearnEnv(schema=schema_path, root_directory=root_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\Uni Stuff\\Bachelor\\RL\\Challenges\\city_learn_2023\\.venv\\lib\\site-packages\\citylearn\\dynamics.py:102: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(self.filepath)['model_state_dict'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed episode: 1/10, Reward: {'min': array([-764.35232653]), 'max': array([0.]), 'sum': array([-58568.38248896]), 'mean': array([-81.45811195])}\n",
      "Completed episode: 2/10, Reward: {'min': array([-680.23664328]), 'max': array([0.]), 'sum': array([-28740.58295231]), 'mean': array([-39.97299437])}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m SAC(env)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic_finish\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Uni Stuff\\Bachelor\\RL\\Challenges\\city_learn_2023\\.venv\\lib\\site-packages\\citylearn\\agents\\base.py:157\u001b[0m, in \u001b[0;36mAgent.learn\u001b[1;34m(self, episodes, deterministic, deterministic_finish, logging_level)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# update\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m deterministic:\n\u001b[1;32m--> 157\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_observations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Uni Stuff\\Bachelor\\RL\\Challenges\\city_learn_2023\\.venv\\lib\\site-packages\\citylearn\\agents\\sac.py:76\u001b[0m, in \u001b[0;36mSAC.update\u001b[1;34m(self, observations, actions, reward, next_observations, done)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Run once the regression model has been fitted\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Normalize all the observations using periodical normalization, one-hot encoding, or -1, 1 scaling. It also removes observations that are not necessary (solar irradiance if there are no solar PV panels).\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (o, a, r, n) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(observations, actions, reward, next_observations)):\n\u001b[1;32m---> 76\u001b[0m     o \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_encoded_observations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_encoded_observations(i, n)\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalized[i]:\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Uni Stuff\\Bachelor\\RL\\Challenges\\city_learn_2023\\.venv\\lib\\site-packages\\citylearn\\agents\\sac.py:231\u001b[0m, in \u001b[0;36mSAC.get_encoded_observations\u001b[1;34m(self, index, observations)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_encoded_observations\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m, observations: List[\u001b[38;5;28mfloat\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mfloat64]:\n\u001b[1;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([j \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39mhstack(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mif\u001b[39;00m j \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m], dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Uni Stuff\\Bachelor\\RL\\Challenges\\city_learn_2023\\.venv\\lib\\site-packages\\citylearn\\preprocessing.py:142\u001b[0m, in \u001b[0;36mNormalize.__mul__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_min \u001b[38;5;241m=\u001b[39m x_min\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_max \u001b[38;5;241m=\u001b[39m x_max\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__mul__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m]):\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_min \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_max:\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = CityLearnEnv(\n",
    "    schema=schema_path, root_directory=root_directory, central_agent=True\n",
    ")\n",
    "model = SAC(env)\n",
    "\n",
    "# train\n",
    "model.learn(episodes=10, deterministic_finish=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\coding\\RLP\\city_learn_2023\\.venv\\lib\\site-packages\\citylearn\\dynamics.py:102: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(self.filepath)['model_state_dict'])\n"
     ]
    }
   ],
   "source": [
    "observations= env.reset()\n",
    "\n",
    "rewards = []\n",
    "while not env.done:\n",
    "    actions= model.predict(observations, deterministic=True)\n",
    "    print(actions)\n",
    "    observations, reward, _, _= env.step(actions)\n",
    "    rewards.append(reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_45348\\337779442.py:26: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "\n",
    "# Smooth the rewards using a uniform filter\n",
    "smoothed_rewards = uniform_filter1d(rewards, size=25, axis=0)  # Adjust size for smoothing level\n",
    "smoothed_decentral_rewards = uniform_filter1d(decentral_rewards, size=25, axis=0)\n",
    "\n",
    "# Plot the smoothed rewards and random rewards\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "plt.plot(rewards, label='Smoothed Rewards Centralized SAC', color='green', linewidth=2)  # Plot smoothed rewards\n",
    "plt.plot(decentral_rewards, label='Smoothed Rewards Decentralized SAC', color='blue', linewidth=2)  # Plot smoothed random rewards\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"Centralized SAC vs. Decentralized SAC Rewards Over Time\", fontsize=16, fontweight='bold')  # Title with larger font size and bold\n",
    "plt.xlabel(\"Time Step\", fontsize=14)  # X-axis label with larger font size\n",
    "plt.ylabel(\"Reward\", fontsize=14)  # Y-axis label with larger font size\n",
    "\n",
    "# Add gridlines for better readability\n",
    "plt.grid(visible=True, linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "# Save and show the plot\n",
    "plt.savefig(\"smoothed_rewards_comparison.png\", dpi=300)  # Save with higher resolution\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
