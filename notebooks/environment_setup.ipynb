{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from citylearn.agents.base import Agent as RandomAgent\n",
    "from citylearn.agents.rbc import HourRBC\n",
    "from citylearn.agents.sac import SAC\n",
    "from citylearn.agents.q_learning import TabularQLearning\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\Uni Stuff\\Bachelor\\RL\\Challenges\\city_learn_2023\\.venv\\lib\\site-packages\\citylearn\\dynamics.py:102: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(self.filepath)['model_state_dict'])\n"
     ]
    }
   ],
   "source": [
    "root_directory = Path(\"../data/citylearn_challenge_2023_phase_1\")\n",
    "schema_path = root_directory / \"schema.json\"\n",
    "env = CityLearnEnv(schema=schema_path, root_directory=root_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed episode: 1/10, Reward: {'min': array([-18.87530494]), 'max': array([0.]), 'sum': array([-3165.6957439]), 'mean': array([-4.4029148])}\n",
      "Completed episode: 2/10, Reward: {'min': array([-18.28673238]), 'max': array([0.]), 'sum': array([-1627.61751414]), 'mean': array([-2.26372394])}\n",
      "Completed episode: 3/10, Reward: {'min': array([-16.84867066]), 'max': array([0.]), 'sum': array([-1522.23806072]), 'mean': array([-2.11716003])}\n",
      "Completed episode: 4/10, Reward: {'min': array([-18.3892653]), 'max': array([0.]), 'sum': array([-1562.48472425]), 'mean': array([-2.17313592])}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m SAC(env)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic_finish\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Uni Stuff\\Bachelor\\RL\\Challenges\\city_learn_2023\\.venv\\lib\\site-packages\\citylearn\\agents\\base.py:149\u001b[0m, in \u001b[0;36mAgent.learn\u001b[1;34m(self, episodes, deterministic, deterministic_finish, logging_level)\u001b[0m\n\u001b[0;32m    146\u001b[0m rewards_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m--> 149\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;66;03m# apply actions to citylearn_env\u001b[39;00m\n\u001b[0;32m    152\u001b[0m     next_observations, rewards, done, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(actions)\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Uni Stuff\\Bachelor\\RL\\Challenges\\city_learn_2023\\.venv\\lib\\site-packages\\citylearn\\agents\\sac.py:186\u001b[0m, in \u001b[0;36mSAC.predict\u001b[1;34m(self, observations, deterministic)\u001b[0m\n\u001b[0;32m    183\u001b[0m deterministic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m deterministic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m deterministic\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_exploration_time_step \u001b[38;5;129;01mor\u001b[39;00m deterministic:\n\u001b[1;32m--> 186\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_post_exploration_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    189\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_exploration_prediction(observations)\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Uni Stuff\\Bachelor\\RL\\Challenges\\city_learn_2023\\.venv\\lib\\site-packages\\citylearn\\agents\\sac.py:203\u001b[0m, in \u001b[0;36mSAC.get_post_exploration_prediction\u001b[1;34m(self, observations, deterministic)\u001b[0m\n\u001b[0;32m    201\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_encoded_observations(i, o)\n\u001b[0;32m    202\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_normalized_observations(i, o)\n\u001b[1;32m--> 203\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    204\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_net[i]\u001b[38;5;241m.\u001b[39msample(o)\n\u001b[0;32m    205\u001b[0m a \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m deterministic \u001b[38;5;28;01melse\u001b[39;00m result[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = CityLearnEnv(\n",
    "    schema=schema_path, root_directory=root_directory, central_agent=True\n",
    ")\n",
    "model = SAC(env)\n",
    "\n",
    "# train\n",
    "model.learn(episodes=10, deterministic_finish=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\coding\\RLP\\city_learn_2023\\.venv\\lib\\site-packages\\citylearn\\dynamics.py:102: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(self.filepath)['model_state_dict'])\n"
     ]
    }
   ],
   "source": [
    "observations= env.reset()\n",
    "\n",
    "rewards = []\n",
    "while not env.done:\n",
    "    actions= model.predict(observations, deterministic=True)\n",
    "    print(actions)\n",
    "    observations, reward, _, _= env.step(actions)\n",
    "    rewards.append(reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_45348\\337779442.py:26: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "\n",
    "# Smooth the rewards using a uniform filter\n",
    "smoothed_rewards = uniform_filter1d(rewards, size=25, axis=0)  # Adjust size for smoothing level\n",
    "smoothed_decentral_rewards = uniform_filter1d(decentral_rewards, size=25, axis=0)\n",
    "\n",
    "# Plot the smoothed rewards and random rewards\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "plt.plot(rewards, label='Smoothed Rewards Centralized SAC', color='green', linewidth=2)  # Plot smoothed rewards\n",
    "plt.plot(decentral_rewards, label='Smoothed Rewards Decentralized SAC', color='blue', linewidth=2)  # Plot smoothed random rewards\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"Centralized SAC vs. Decentralized SAC Rewards Over Time\", fontsize=16, fontweight='bold')  # Title with larger font size and bold\n",
    "plt.xlabel(\"Time Step\", fontsize=14)  # X-axis label with larger font size\n",
    "plt.ylabel(\"Reward\", fontsize=14)  # Y-axis label with larger font size\n",
    "\n",
    "# Add gridlines for better readability\n",
    "plt.grid(visible=True, linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "# Save and show the plot\n",
    "plt.savefig(\"smoothed_rewards_comparison.png\", dpi=300)  # Save with higher resolution\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
